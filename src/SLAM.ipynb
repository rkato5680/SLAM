{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bresenham2D(sx, sy, ex, ey):\n",
    "  '''\n",
    "  Bresenham's ray tracing algorithm in 2D.\n",
    "  Inputs:\n",
    "\t  (sx, sy)\tstart point of ray\n",
    "\t  (ex, ey)\tend point of ray\n",
    "  '''\n",
    "  sx = int(round(sx))\n",
    "  sy = int(round(sy))\n",
    "  ex = int(round(ex))\n",
    "  ey = int(round(ey))\n",
    "  dx = abs(ex-sx)\n",
    "  dy = abs(ey-sy)\n",
    "  steep = abs(dy)>abs(dx)\n",
    "  if steep:\n",
    "    dx,dy = dy,dx # swap \n",
    "\n",
    "  if dy == 0:\n",
    "    q = np.zeros((dx+1,1),dtype=int)\n",
    "  else:\n",
    "    q = np.append(0,np.greater_equal(np.diff(np.mod(np.arange( np.floor(dx/2), -dy*dx+np.floor(dx/2)-1,-dy),dx)),0))\n",
    "  if steep:\n",
    "    if sy <= ey:\n",
    "      y = np.arange(sy,ey+1)\n",
    "    else:\n",
    "      y = np.arange(sy,ey-1,-1)\n",
    "    if sx <= ex:\n",
    "      x = sx + np.cumsum(q)\n",
    "    else:\n",
    "      x = sx - np.cumsum(q)\n",
    "  else:\n",
    "    if sx <= ex:\n",
    "      x = np.arange(sx,ex+1)\n",
    "    else:\n",
    "      x = np.arange(sx,ex-1,-1)\n",
    "    if sy <= ey:\n",
    "      y = sy + np.cumsum(q)\n",
    "    else:\n",
    "      y = sy - np.cumsum(q)\n",
    "  return np.vstack((x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading csv files\n",
    "df_encoder = pd.read_csv(\"../sensor_data/encoder.csv\", header=None)\n",
    "df_encoder.columns =  [\"timestamp\", \"left count\", \"right count\"]\n",
    "\n",
    "df_fog = pd.read_csv(\"../sensor_data/fog.csv\", header=None)\n",
    "df_fog.columns =  [\"timestamp\", \"delta roll\", \"delta pitch\", \"delta yaw\"]\n",
    "\n",
    "df_lidar = pd.read_csv(\"../sensor_data/lidar.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading R matrix and T vector for tranforming lidar frame to the world frame\n",
    "R = list(map(float, \"0.00130201 0.796097 0.605167 0.999999 -0.000419027 -0.00160026 -0.00102038 0.605169 -0.796097\".split()))\n",
    "R = np.array(R).reshape(3,3)\n",
    "\n",
    "T = list(map(float, \"0.8349 -0.0126869 1.76416\".split()))\n",
    "T = np.array(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniforge/base/lib/python3.8/site-packages/pandas/core/indexing.py:1675: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(ilocs[0], value, pi)\n"
     ]
    }
   ],
   "source": [
    "# creating dataframe combinining all necessary information for each time step\n",
    "\n",
    "start = 1544582648800000000\n",
    "end =   1544583809200000000\n",
    "#stepsize\n",
    "step = 1000000000\n",
    "\n",
    "diam_left = 0.623479\n",
    "diam_right = 0.622806\n",
    "\n",
    "df_tmp = df_encoder[np.logical_and((start<=df_encoder.iloc[:, 0]), (df_encoder.iloc[:, 0]<=end))]\n",
    "df_tmp.iloc[:, 1] *= diam_left*np.pi/4096\n",
    "df_tmp.iloc[:, 2] *= diam_right*np.pi/4096\n",
    "\n",
    "df2 = pd.DataFrame(df_tmp.iloc[1:, 0])\n",
    "df2[\"timedelta\"] = df_tmp.iloc[:, 0].diff()[1:] / 10**9 #ここで秒に戻す\n",
    "df2[\"delta_x\"] = df_tmp.iloc[:, 1:].mean(1).diff()\n",
    "df2 = df2.reset_index(drop=True)\n",
    "\n",
    "df_tmp0 = df_fog[np.logical_and((start<=df_fog.iloc[:, 0]), (df_fog.iloc[:, 0]<=end))]\n",
    "df_tmp = pd.DataFrame(df_tmp0.iloc[:, 0])\n",
    "df_tmp[\"yaw\"] = df_tmp0.iloc[:, 3].cumsum()\n",
    "df2[\"delta_yaw\"] = df_tmp[\"yaw\"][::10].diff().iloc[1:len(df2)+1].to_numpy()\n",
    "\n",
    "df2[\"v\"] = df2[\"delta_x\"] / df2[\"timedelta\"]\n",
    "df2[\"omega\"] = df2[\"delta_yaw\"] / df2[\"timedelta\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing initial value of mean and covariance for prediciton of each particel\n",
    "# equivalent to sample mean and covariance\n",
    "df_tmp = df2[np.logical_and((start<=df2.iloc[:, 0]), (df2.iloc[:, 0]<=start+step))]\n",
    "mean = df_tmp.iloc[:, 4:].mean(0)\n",
    "cov = df_tmp.iloc[:, 4:].cov()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 -th iteration done...\n",
      "200 -th iteration done...\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 200 is out of bounds for axis 1 with size 200",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-f4c7e24616d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mm_tmp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbresenham2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my0\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m                 \u001b[0mgrid_tmp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mys\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0;31m# computing map correlation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 200 is out of bounds for axis 1 with size 200"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "n_particle = 100\n",
    "gridmap = np.zeros((1500,1400))\n",
    "rmin,rmax =2,75 #cutoff of Lidar data\n",
    "log4 = np.log(4)\n",
    "lmin,lmax = -5,5 #min and max of cumulative log-odds\n",
    "ldecay = 0.999 # decay of log-odds\n",
    "dx,dy = 100,1200 # adjustment to grid map coordinate\n",
    "scale_pa_var = np.array([[1,1],[1,1]]) # scaling factor of covariance\n",
    "\n",
    "# resampling parameters\n",
    "thr_v_resampling = 1/n_particle / 2\n",
    "thr_n_resampling = n_particle // 5\n",
    "\n",
    "gridmap_list = []\n",
    "\n",
    "n_step = (end - start + step - 1)//step\n",
    "\n",
    "# pos[i, j, k]: estimated position (x,y,theta) for i-th time step and j-th particle\n",
    "pos_pa = np.zeros((n_step, n_particle, 3))\n",
    "weights = np.zeros((n_step, n_particle))\n",
    "weights[0] = np.ones(n_particle) / n_particle\n",
    "\n",
    "# estimated position by summing esitimation of paricles\n",
    "pos = np.zeros((n_step, 3))\n",
    "\n",
    "# observed position\n",
    "pos_ob = np.zeros((n_step, 3))\n",
    "\n",
    "tau = step / 10**9\n",
    "for i, s in enumerate(range(start+step, end, step)):\n",
    "    # prediction of each particle\n",
    "    v,omega = np.random.multivariate_normal(mean, cov * scale_pa_var, n_particle).T\n",
    "    \n",
    "    theta = pos_pa[i, :, 2] + omega * tau\n",
    "    x = pos_pa[i, :, 0] + v*np.cos(theta) * tau\n",
    "    y = pos_pa[i, :, 1] + v*np.sin(theta) *tau\n",
    "    pos_pa[i+1, :, :] = np.vstack([x,y,theta]).T\n",
    "    \n",
    "    # search for nearlest Lidar data in term of timestamp using binary search\n",
    "    middle = s + step//2\n",
    "    idx = np.searchsorted(df_lidar.iloc[:, 0], middle)\n",
    "    tmp = df_lidar.iloc[idx, 1:]\n",
    "    tmp = tmp[np.logical_and((rmin<=tmp), (tmp<=rmax))]\n",
    "    indices = tmp.index\n",
    "    args = (indices*0.666 + -5)/180*np.pi\n",
    "\n",
    "    m = np.vstack([tmp*np.cos(args), tmp*np.sin(args)]).T\n",
    "    \n",
    "    # coordinate transformation\n",
    "    tmp = np.zeros((m.shape[0], 3))\n",
    "    tmp[:, :2] = m\n",
    "    m = (R @ (tmp+T).T).T[:, :2]\n",
    "    \n",
    "    # Computing weight of particles using map correlation\n",
    "    if i == 0: weights[i+1, :] = weights[i, :].copy()\n",
    "    else:\n",
    "        corrs = np.zeros(n_particle, int)\n",
    "        for pa_idx, (x,y,theta) in enumerate(pos_pa[i+1]):\n",
    "            rot_mat = np.array([[np.cos(theta), -np.sin(theta)], [np.sin(theta), np.cos(theta)]])\n",
    "            m_pa = (rot_mat @ m.T).T\n",
    "            m_pa[:, 0] += x\n",
    "            m_pa[:, 1] += y\n",
    "            x0,y0 = int(x)+dx,int(y)+dy\n",
    "            m_pa = m_pa.astype(int)\n",
    "            m_pa[:, 0] += dx\n",
    "            m_pa[:, 1] += dy\n",
    "\n",
    "            # To avoid complexity, only compare correlation of negative values\n",
    "            # To avoid complexity, only compare 200 x 200 sized small grid\n",
    "            grid_tmp = np.zeros((200,200), int)\n",
    "            for x,y in m_tmp:\n",
    "                xs,ys = bresenham2D(100,100,x-(x0-100),y-(y0-100))\n",
    "                grid_tmp[xs,ys] = -1\n",
    "\n",
    "            # computing map correlation\n",
    "            corrs[pa_idx] = np.count_nonzero(np.logical_and((gridmap[x0-100:x0+100,y0-100:y0+100]<0), (grid_tmp<0)))\n",
    "\n",
    "        weights[i+1, :] = corrs / corrs.sum()\n",
    "    \n",
    "    # computing single estimated position by summing predictions of particles\n",
    "    pos[i+1,:] = (pos_pa[i+1, :, :] * weights[i+1, :, None]).sum(0)\n",
    "    \n",
    "    # transform Lidar data to the world frame\n",
    "    theta = pos[i+1,2]\n",
    "    rot_mat = np.array([[np.cos(theta), -np.sin(theta)], [np.sin(theta), np.cos(theta)]])\n",
    "    m = (rot_mat @ m.T).T\n",
    "    \n",
    "    m[:, 0] += pos[i+1, 0]\n",
    "    m[:, 1] += pos[i+1, 1]\n",
    "    x0,y0 = int(pos[i+1, 0])+dx,int(pos[i+1, 1])+dy\n",
    "    m = m.astype(int)\n",
    "    m[:, 0] += dx\n",
    "    m[:, 1] += dy\n",
    "    \n",
    "    m_tmp = sorted(m, key=lambda x: (abs(x[0]-x0), abs(x[1]-y0)), reverse=True)\n",
    "    \n",
    "    # To avoid overwrapping, aggreage unoccupied and occupied cells in temporary grid and then add it to the original grid map.\n",
    "    grid_tmp = np.zeros(gridmap.shape)\n",
    "    for x,y in m_tmp:\n",
    "        xs,ys = bresenham2D(x0,y0,x,y)\n",
    "        grid_tmp[xs,ys] = -log4\n",
    "        grid_tmp[x,y] = log4\n",
    "        \n",
    "    gridmap += grid_tmp\n",
    "    \n",
    "    # maintaining log-odds in small range\n",
    "    gridmap[gridmap < lmin] = lmin\n",
    "    gridmap[gridmap > lmax] = lmax\n",
    "    \n",
    "    if i and i % 100 == 0:\n",
    "        print(f\"{i} -th iteration done...\")\n",
    "        gridmap_list.append(gridmap.copy())\n",
    "        \n",
    "    gridmap *= ldecay\n",
    "    \n",
    "    # Computing sample mean and covariance of linear velocity and angular velocity for prediction in the next time step\n",
    "    df_tmp = df2[np.logical_and((s<=df2.iloc[:, 0]), (df2.iloc[:, 0]<=s+step))]\n",
    "    mean = df_tmp.iloc[:, 4:].mean(0)\n",
    "    cov = df_tmp.iloc[:, 4:].cov()\n",
    "    \n",
    "    # Computing position using observed data\n",
    "    # it is not used for prediction\n",
    "    v_ob = df_tmp[\"v\"].mean()\n",
    "    omega_ob = df_tmp[\"omega\"].mean()\n",
    "    \n",
    "    theta_ob = pos_ob[i,2] + omega_ob * tau\n",
    "    x_ob = pos_ob[i,0] + v_ob*np.cos(theta_ob) * tau\n",
    "    y_ob = pos_ob[i,1] + v_ob*np.sin(theta_ob) *tau\n",
    "    pos_ob[i+1, :] = [x_ob, y_ob, theta_ob]\n",
    "    \n",
    "    # resampling\n",
    "    n_bellow = np.count_nonzero(weights[i+1,:] < thr_v_resampling)\n",
    "    if n_bellow >= thr_n_resampling:\n",
    "        indices = np.random.choice(n_particle, n_particle, p=weights[i +1, :])\n",
    "        pos_pa[i+1,:,:] = pos_pa[i+1,indices,:]\n",
    "        weights[i+1,:] = weights[i+1, indices]\n",
    "        weights[i+1] /= weights[i+1].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting\n",
    "\n",
    "for i, gm in enumerate(gridmap_list):\n",
    "    if i == 0 or i % 10: continue\n",
    "    idx = (i+1)*100\n",
    "    vmin,vmax = 0,2\n",
    "    plt.figure(figsize=(20,20))\n",
    "    grid_tmp = 1-(gm<0)\n",
    "    grid_tmp[grid_tmp==vmin] = vmax\n",
    "    plt.imshow(grid_tmp.T, cmap='gray', vmin=vmin, vmax=vmax,origin=\"lower\")\n",
    "\n",
    "    plt.plot(pos[:idx, 0]+dx, pos[:idx, 1]+dy, color=\"green\",label=\"trajectory\", linewidth=3)\n",
    "\n",
    "    plt.scatter(pos_pa[:idx, :10, 0].flatten()+dx, pos_pa[:idx, :10, 1].flatten()+dy,color=\"red\", s=1, label=\"selected particles\")\n",
    "    \n",
    "    plt.scatter(0,0,color=\"grey\", label=f\"T = {int((start + (idx + 1) * step)/10**9)}\", s=0.001)\n",
    "\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.legend(prop={'size': 30}, markerscale=8)\n",
    "    plt.show()\n",
    "    \n",
    "idx = (i+1)*100\n",
    "vmin,vmax = 0,2\n",
    "plt.figure(figsize=(20,20))\n",
    "grid_tmp = 1-(gridmap<0)\n",
    "grid_tmp[grid_tmp==vmin] = vmax\n",
    "plt.imshow(grid_tmp.T, cmap='gray', vmin=vmin, vmax=vmax,origin=\"lower\")\n",
    "\n",
    "plt.plot(pos[:idx, 0]+dx, pos[:idx, 1]+dy, color=\"green\",label=\"trajectory\", linewidth=3)\n",
    "\n",
    "plt.scatter(pos_pa[:idx, :10, 0].flatten()+dx, pos_pa[:idx, :10, 1].flatten()+dy,color=\"red\", s=1, label=\"selected particles\")\n",
    "\n",
    "plt.scatter(0,0,color=\"grey\", label=f\"T = {int((start + (idx + 1) * step)/10**9)}\", s=0.001)\n",
    "\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.legend(prop={'size': 30}, markerscale=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving results\n",
    "\n",
    "#count_save = 0\n",
    "\n",
    "import pickle\n",
    "\n",
    "result = {\"parameters\": [n_particle, thr_v_resampling, thr_n_resampling, scale_pa_var],\n",
    "    \"grid\": gridmap, \n",
    "          \"grid_list\": gridmap_list,\n",
    "          \"estimated_position\": pos, \n",
    "          \"positions of particles\": pos_pa, \n",
    "          \"weight\": weights}\n",
    "\n",
    "with open(f'result_{count_save}.pickle', 'wb') as handle:\n",
    "    pickle.dump(result, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "count_save += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
